{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 1  \n",
    "\n",
    "Обучите нейронную сеть решать шифр цезаря.  \n",
    "Что необходимо сделать:  \n",
    "1.Написать алгоритм шифра цезаря для генерации выборки (сдвиг на К каждой буквы. Например, при сдвиге на 2 буква “А” переходит в букву “В” и тп)  \n",
    "2.Сделать нейронную сеть  \n",
    "3.Обучить ее (вход - зашифрованная фраза, выход - дешифрованная фраза)  \n",
    "4.Проверить качество  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Написать алгоритм шифра цезаря для генерации выборки (сдвиг на К каждой буквы. Например, при сдвиге на 2 буква “А” переходит в букву “В” и тп)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 10\n",
    "vocab = [char for char in 'АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ']\n",
    "\n",
    "\n",
    "def encrypt(text):\n",
    "    indexes = [vocab.index(char) for char in text]\n",
    "    encrypted_indexes = [(idx + key) % len(vocab) for idx in indexes]\n",
    "    encrypted_chars = [vocab[idx] for idx in encrypted_indexes]\n",
    "    encrypted = ''.join(encrypted_chars)\n",
    "    return encrypted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯАБВГДЕЁЖЗИ\n"
     ]
    }
   ],
   "source": [
    "print(encrypt('АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 128\n",
    "message_length = 32\n",
    "\n",
    "\n",
    "def dataset(num_examples):\n",
    "    dataset = []\n",
    "    for x in range(num_examples):\n",
    "        ex_out = ''.join([random.choice(vocab) for x in range(message_length)])\n",
    "        ex_in = encrypt(''.join(ex_out))\n",
    "        ex_in = [vocab.index(x) for x in ex_in]\n",
    "        ex_out = [vocab.index(x) for x in ex_out]\n",
    "        dataset.append([torch.tensor(ex_in), torch.tensor(ex_out)])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Сделать нейронную сеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "hidden_dim = 10\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "embed = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
    "linear = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "softmax = torch.nn.functional.softmax\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(list(embed.parameters()) +\n",
    "                             list(lstm.parameters()) +\n",
    "                             list(linear.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_hidden():\n",
    "    return (torch.zeros(1, 1, hidden_dim),\n",
    "            torch.zeros(1, 1, hidden_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Обучить ее (вход - зашифрованная фраза, выход - дешифрованная фраза)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: 3.0130\n",
      "Epoch: 1\n",
      "Loss: 1.9267\n",
      "Epoch: 2\n",
      "Loss: 1.1917\n",
      "Epoch: 3\n",
      "Loss: 0.5336\n",
      "Epoch: 4\n",
      "Loss: 0.4140\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "accuracies, max_accuracy = [], 0\n",
    "for x in range(num_epochs):\n",
    "    print('Epoch: {}'.format(x))\n",
    "    for encrypted, original in dataset(num_examples):\n",
    "        lstm_in = embed(encrypted)\n",
    "        lstm_in = lstm_in.unsqueeze(1)\n",
    "        lstm_out, lstm_hidden = lstm(lstm_in, zero_hidden())\n",
    "        scores = linear(lstm_out)\n",
    "        scores = scores.transpose(1, 2)\n",
    "        original = original.unsqueeze(1)\n",
    "        loss = loss_fn(scores, original) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Loss: {:6.4f}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Проверить качество "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.92%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        matches, total = 0, 0\n",
    "        for encrypted, original in dataset(num_examples):\n",
    "            lstm_in = embed(encrypted)\n",
    "            lstm_in = lstm_in.unsqueeze(1)\n",
    "            lstm_out, lstm_hidden = lstm(lstm_in, zero_hidden())\n",
    "            scores = linear(lstm_out)\n",
    "            predictions = softmax(scores, dim=2)\n",
    "            _, batch_out = predictions.max(dim=2)\n",
    "            batch_out = batch_out.squeeze(1)\n",
    "            matches += torch.eq(batch_out, original).sum().item()\n",
    "            total += torch.numel(batch_out)\n",
    "        accuracy = matches / total\n",
    "        print('Accuracy: {:4.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "hidden_dim = 10\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        ## Здесь создать слои\n",
    "        self.embed = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = torch.nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, sentences, state=None):\n",
    "        ## Здесь применить\n",
    "        embed = self.embed(sentences)\n",
    "        o, s = self.rnn(embed)\n",
    "        out = self.linear(o)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Time: 0.259, Train loss: 3.201\n",
      "Epoch 1. Time: 0.249, Train loss: 2.512\n",
      "Epoch 2. Time: 0.257, Train loss: 1.879\n",
      "Epoch 3. Time: 0.278, Train loss: 1.398\n",
      "Epoch 4. Time: 0.265, Train loss: 1.043\n",
      "Epoch 5. Time: 0.271, Train loss: 0.802\n",
      "Epoch 6. Time: 0.264, Train loss: 0.630\n",
      "Epoch 7. Time: 0.269, Train loss: 0.503\n",
      "Epoch 8. Time: 0.263, Train loss: 0.417\n",
      "Epoch 9. Time: 0.249, Train loss: 0.347\n"
     ]
    }
   ],
   "source": [
    "for ep in range(10):\n",
    "    start = time.time()\n",
    "    train_loss = 0.\n",
    "    train_passed = 0\n",
    "\n",
    "    for encrypted, original in dataset(num_examples):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        answers = model.forward(encrypted.unsqueeze(1))\n",
    "        answers = answers.view(-1, vocab_size)\n",
    "        loss = criterion(answers, original)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_passed += 1\n",
    "\n",
    "    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        matches, total = 0, 0\n",
    "        for encrypted, original in dataset(num_examples):\n",
    "            answers = model.forward(encrypted.unsqueeze(1))\n",
    "            predictions = torch.nn.functional.softmax(answers, dim=2)\n",
    "            _, batch_out = predictions.max(dim=2)\n",
    "            batch_out = batch_out.squeeze(1)\n",
    "            matches += torch.eq(batch_out, original).sum().item()\n",
    "            total += torch.numel(batch_out)\n",
    "        accuracy = matches / total\n",
    "        print('Accuracy: {:4.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### Задание 2.  \n",
    "\n",
    "Выполнить практическую работу из лекционного ноутбука.  \n",
    "а) построить RNN-ячейку на основе полносвязных слоев  \n",
    "б) применить построенную ячейку для генерации текста с выражениями героев сериала “Симпсоны”  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "##### Препроцессинг данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from string import ascii_lowercase\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>timestamp_in_ms</th>\n",
       "      <th>speaking_line</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>raw_location_text</th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10368</td>\n",
       "      <td>35</td>\n",
       "      <td>29</td>\n",
       "      <td>Lisa Simpson: Maggie, look. What's that?</td>\n",
       "      <td>235000</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Simpson Home</td>\n",
       "      <td>Maggie, look. What's that?</td>\n",
       "      <td>maggie look whats that</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10369</td>\n",
       "      <td>35</td>\n",
       "      <td>30</td>\n",
       "      <td>Lisa Simpson: Lee-mur. Lee-mur.</td>\n",
       "      <td>237000</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Simpson Home</td>\n",
       "      <td>Lee-mur. Lee-mur.</td>\n",
       "      <td>lee-mur lee-mur</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10370</td>\n",
       "      <td>35</td>\n",
       "      <td>31</td>\n",
       "      <td>Lisa Simpson: Zee-boo. Zee-boo.</td>\n",
       "      <td>239000</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Simpson Home</td>\n",
       "      <td>Zee-boo. Zee-boo.</td>\n",
       "      <td>zee-boo zee-boo</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10372</td>\n",
       "      <td>35</td>\n",
       "      <td>33</td>\n",
       "      <td>Lisa Simpson: I'm trying to teach Maggie that ...</td>\n",
       "      <td>245000</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Simpson Home</td>\n",
       "      <td>I'm trying to teach Maggie that nature doesn't...</td>\n",
       "      <td>im trying to teach maggie that nature doesnt e...</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10374</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>Lisa Simpson: It's like an ox, only it has a h...</td>\n",
       "      <td>254000</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Simpson Home</td>\n",
       "      <td>It's like an ox, only it has a hump and a dewl...</td>\n",
       "      <td>its like an ox only it has a hump and a dewlap...</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id  episode_id  number  \\\n",
       "0           0  10368          35      29   \n",
       "1           1  10369          35      30   \n",
       "2           2  10370          35      31   \n",
       "3           3  10372          35      33   \n",
       "4           4  10374          35      35   \n",
       "\n",
       "                                            raw_text  timestamp_in_ms  \\\n",
       "0           Lisa Simpson: Maggie, look. What's that?           235000   \n",
       "1                    Lisa Simpson: Lee-mur. Lee-mur.           237000   \n",
       "2                    Lisa Simpson: Zee-boo. Zee-boo.           239000   \n",
       "3  Lisa Simpson: I'm trying to teach Maggie that ...           245000   \n",
       "4  Lisa Simpson: It's like an ox, only it has a h...           254000   \n",
       "\n",
       "   speaking_line  character_id  location_id raw_character_text  \\\n",
       "0           True             9          5.0       Lisa Simpson   \n",
       "1           True             9          5.0       Lisa Simpson   \n",
       "2           True             9          5.0       Lisa Simpson   \n",
       "3           True             9          5.0       Lisa Simpson   \n",
       "4           True             9          5.0       Lisa Simpson   \n",
       "\n",
       "  raw_location_text                                       spoken_words  \\\n",
       "0      Simpson Home                         Maggie, look. What's that?   \n",
       "1      Simpson Home                                  Lee-mur. Lee-mur.   \n",
       "2      Simpson Home                                  Zee-boo. Zee-boo.   \n",
       "3      Simpson Home  I'm trying to teach Maggie that nature doesn't...   \n",
       "4      Simpson Home  It's like an ox, only it has a hump and a dewl...   \n",
       "\n",
       "                                     normalized_text  word_count  \n",
       "0                             maggie look whats that         4.0  \n",
       "1                                    lee-mur lee-mur         2.0  \n",
       "2                                    zee-boo zee-boo         2.0  \n",
       "3  im trying to teach maggie that nature doesnt e...        24.0  \n",
       "4  its like an ox only it has a hump and a dewlap...        18.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = df['normalized_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11639"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['maggie look whats that',\n",
       " 'lee-mur lee-mur',\n",
       " 'zee-boo zee-boo',\n",
       " 'im trying to teach maggie that nature doesnt end with the barnyard i want her to have all the advantages that i didnt have',\n",
       " 'its like an ox only it has a hump and a dewlap hump and dew-lap hump and dew-lap',\n",
       " 'you know his blood type how romantic',\n",
       " 'oh yeah whats my shoe size',\n",
       " 'ring',\n",
       " 'yes dad',\n",
       " 'ooh look maggie what is that do-dec-ah-edron dodecahedron']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [[char for char in sent] for sent in sents if isinstance(sent, str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m',\n",
       " 'a',\n",
       " 'g',\n",
       " 'g',\n",
       " 'i',\n",
       " 'e',\n",
       " ' ',\n",
       " 'l',\n",
       " 'o',\n",
       " 'o',\n",
       " 'k',\n",
       " ' ',\n",
       " 'w',\n",
       " 'h',\n",
       " 'a',\n",
       " 't',\n",
       " 's',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'a',\n",
       " 't']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_TO_INDEX = {w: i for i, w in enumerate(ascii_lowercase, 1)}\n",
    "CHAR_TO_INDEX[' '] = 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем среднюю длину реплики, чтобы выяснить, до какой длины делать padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.36635754292535"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(sent) for sent in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "лучше до 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50\n",
    "\n",
    "X = torch.zeros((len(text), MAX_LEN), dtype=int)\n",
    "for i in range(len(text)):\n",
    "    for j, w in enumerate(text[i]):\n",
    "        if j >= MAX_LEN:\n",
    "            break\n",
    "        if w.lower() in CHAR_TO_INDEX:\n",
    "            X[i][j] = CHAR_TO_INDEX[w.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13,  1,  7,  7,  9,  5, 27, 12, 15, 15, 11, 27, 23,  8,  1, 20, 19, 27,\n",
       "         20,  8,  1, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [12,  5,  5,  0, 13, 21, 18, 27, 12,  5,  5,  0, 13, 21, 18,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [26,  5,  5,  0,  2, 15, 15, 27, 26,  5,  5,  0,  2, 15, 15,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 9, 13, 27, 20, 18, 25,  9, 14,  7, 27, 20, 15, 27, 20,  5,  1,  3,  8,\n",
       "         27, 13,  1,  7,  7,  9,  5, 27, 20,  8,  1, 20, 27, 14,  1, 20, 21, 18,\n",
       "          5, 27,  4, 15,  5, 19, 14, 20, 27,  5, 14,  4, 27, 23],\n",
       "        [ 9, 20, 19, 27, 12,  9, 11,  5, 27,  1, 14, 27, 15, 24, 27, 15, 14, 12,\n",
       "         25, 27,  9, 20, 27,  8,  1, 19, 27,  1, 27,  8, 21, 13, 16, 27,  1, 14,\n",
       "          4, 27,  1, 27,  4,  5, 23, 12,  1, 16, 27,  8, 21, 13],\n",
       "        [25, 15, 21, 27, 11, 14, 15, 23, 27,  8,  9, 19, 27,  2, 12, 15, 15,  4,\n",
       "         27, 20, 25, 16,  5, 27,  8, 15, 23, 27, 18, 15, 13,  1, 14, 20,  9,  3,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [15,  8, 27, 25,  5,  1,  8, 27, 23,  8,  1, 20, 19, 27, 13, 25, 27, 19,\n",
       "          8, 15,  5, 27, 19,  9, 26,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [18,  9, 14,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [25,  5, 19, 27,  4,  1,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [15, 15,  8, 27, 12, 15, 15, 11, 27, 13,  1,  7,  7,  9,  5, 27, 23,  8,\n",
       "          1, 20, 27,  9, 19, 27, 20,  8,  1, 20, 27,  4, 15,  0,  4,  5,  3,  0,\n",
       "          1,  8,  0,  5,  4, 18, 15, 14, 27,  4, 15,  4,  5,  3]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 25\n",
    "\n",
    "y = torch.zeros((len(text), MAX_LEN), dtype=int)\n",
    "for i in range(len(text)):\n",
    "    for j, w in enumerate(text[i]):\n",
    "        if j >= MAX_LEN:\n",
    "            break\n",
    "        if w.lower() in CHAR_TO_INDEX:\n",
    "            y[i][j] = CHAR_TO_INDEX[w.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13,  1,  7,  7,  9,  5, 27, 12, 15, 15, 11, 27, 23,  8,  1, 20, 19, 27,\n",
       "         20,  8,  1, 20,  0,  0,  0],\n",
       "        [12,  5,  5,  0, 13, 21, 18, 27, 12,  5,  5,  0, 13, 21, 18,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0],\n",
       "        [26,  5,  5,  0,  2, 15, 15, 27, 26,  5,  5,  0,  2, 15, 15,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 9, 13, 27, 20, 18, 25,  9, 14,  7, 27, 20, 15, 27, 20,  5,  1,  3,  8,\n",
       "         27, 13,  1,  7,  7,  9,  5],\n",
       "        [ 9, 20, 19, 27, 12,  9, 11,  5, 27,  1, 14, 27, 15, 24, 27, 15, 14, 12,\n",
       "         25, 27,  9, 20, 27,  8,  1],\n",
       "        [25, 15, 21, 27, 11, 14, 15, 23, 27,  8,  9, 19, 27,  2, 12, 15, 15,  4,\n",
       "         27, 20, 25, 16,  5, 27,  8],\n",
       "        [15,  8, 27, 25,  5,  1,  8, 27, 23,  8,  1, 20, 19, 27, 13, 25, 27, 19,\n",
       "          8, 15,  5, 27, 19,  9, 26],\n",
       "        [18,  9, 14,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0],\n",
       "        [25,  5, 19, 27,  4,  1,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0],\n",
       "        [15, 15,  8, 27, 12, 15, 15, 11, 27, 13,  1,  7,  7,  9,  5, 27, 23,  8,\n",
       "          1, 20, 27,  9, 19, 27, 20]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Архитектура RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerationNetwork(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(TextGenerationNetwork, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = torch.nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.dense = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, train=True, state=None):\n",
    "        with torch.set_grad_enabled(train):\n",
    "            out = self.embedding(x)\n",
    "            out, state = self.rnn(out)\n",
    "            out = self.dense(out)\n",
    "            out = out.squeeze(0)\n",
    "        return out, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавим ограничение на длину сгенерированного предложения\n",
    "\n",
    "def generate_sentence(model):\n",
    "    sent = 'hello'\n",
    "    id = -1\n",
    "    while id != 0 and len(sent) < MAX_LEN:\n",
    "        sent_as_tensor = [torch.as_tensor(\n",
    "            np.array([[CHAR_TO_INDEX[char]]]),\n",
    "            dtype=torch.long) for char in sent]\n",
    "        state = None\n",
    "        for char in sent_as_tensor:\n",
    "            out, state = model.forward(char, train=False, state=state)\n",
    "        id = np.argmax(out.numpy().flatten())\n",
    "        if id == 27:\n",
    "            sent += ' '\n",
    "        else:\n",
    "            sent += ascii_lowercase[id - 1]\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Обучение и генерация предложений на каждой эпохе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 28\n",
    "embedding_dim = 28\n",
    "hidden_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextGenerationNetwork(vocab_size, embedding_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n",
    "batch_size = 100\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c5340e2ecb4f5a80a49ea63434670e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, time: 5.1s, train loss: 1.608\n",
      "hellouthe we we we we we \n",
      "\n",
      "Epoch: 2, time: 4.9s, train loss: 1.596\n",
      "hellouthe we we we we we \n",
      "\n",
      "Epoch: 3, time: 5.0s, train loss: 1.584\n",
      "hellouthe we we we we we \n",
      "\n",
      "Epoch: 4, time: 5.1s, train loss: 1.574\n",
      "hellouthe we we we we we \n",
      "\n",
      "Epoch: 5, time: 5.2s, train loss: 1.564\n",
      "hellouthe we we we we we \n",
      "\n",
      "Epoch: 6, time: 5.3s, train loss: 1.554\n",
      "hellouthe we we we we we \n",
      "\n",
      "Epoch: 7, time: 4.9s, train loss: 1.545\n",
      "hellouthe we we we we we \n",
      "\n",
      "Epoch: 8, time: 4.0s, train loss: 1.537\n",
      "hellouthe we we we we we \n",
      "\n",
      "Epoch: 9, time: 3.1s, train loss: 1.529\n",
      "hellouthe we we we we we \n",
      "\n",
      "Epoch: 10, time: 4.3s, train loss: 1.521\n",
      "helloulllllllllllllllllll\n",
      "\n",
      "Epoch: 11, time: 3.9s, train loss: 1.513\n",
      "helloulllllllllllllllllll\n",
      "\n",
      "Epoch: 12, time: 4.5s, train loss: 1.506\n",
      "helloulllllllllllllllllll\n",
      "\n",
      "Epoch: 13, time: 2.8s, train loss: 1.500\n",
      "helloulllllllllllllllllll\n",
      "\n",
      "Epoch: 14, time: 2.7s, train loss: 1.493\n",
      "helloulllllllllllllllllll\n",
      "\n",
      "Epoch: 15, time: 3.3s, train loss: 1.487\n",
      "helloulllllllllllllllllll\n",
      "\n",
      "Epoch: 16, time: 5.2s, train loss: 1.480\n",
      "helloulllllllllllllllllll\n",
      "\n",
      "Epoch: 17, time: 2.7s, train loss: 1.475\n",
      "helloulllllllllllllllllll\n",
      "\n",
      "Epoch: 18, time: 4.0s, train loss: 1.469\n",
      "helloulllllllllllllllllll\n",
      "\n",
      "Epoch: 19, time: 3.7s, train loss: 1.463\n",
      "helloulllllllllllllllllll\n",
      "\n",
      "Epoch: 20, time: 3.0s, train loss: 1.458\n",
      "helloulllllllllllllllllll\n",
      "\n",
      "Epoch: 21, time: 3.2s, train loss: 1.453\n",
      "helloulllllllllllllllllll\n",
      "\n",
      "Epoch: 22, time: 2.9s, train loss: 1.448\n",
      "helloulllllllllllllllllll\n",
      "\n",
      "Epoch: 23, time: 2.9s, train loss: 1.443\n",
      "helloulanoulanoulanoulano\n",
      "\n",
      "Epoch: 24, time: 2.8s, train loss: 1.438\n",
      "helloulanoulanoulanoulano\n",
      "\n",
      "Epoch: 25, time: 2.9s, train loss: 1.433\n",
      "helloulanoulanoulanoulano\n",
      "\n",
      "Epoch: 26, time: 3.2s, train loss: 1.428\n",
      "helloulanoulanoulanoulano\n",
      "\n",
      "Epoch: 27, time: 2.7s, train loss: 1.424\n",
      "helloulanoulanoulanoulano\n",
      "\n",
      "Epoch: 28, time: 2.7s, train loss: 1.420\n",
      "helloulanoulanoulanoulano\n",
      "\n",
      "Epoch: 29, time: 2.9s, train loss: 1.415\n",
      "helloulanoulanoulanoulano\n",
      "\n",
      "Epoch: 30, time: 3.1s, train loss: 1.411\n",
      "helloulanoulanoulanoulano\n",
      "\n",
      "Epoch: 31, time: 3.2s, train loss: 1.407\n",
      "helloulanoulanoulanoulano\n",
      "\n",
      "Epoch: 32, time: 2.7s, train loss: 1.403\n",
      "helloulanoulanoulanoulano\n",
      "\n",
      "Epoch: 33, time: 2.9s, train loss: 1.399\n",
      "helloulanoulanoulanoulano\n",
      "\n",
      "Epoch: 34, time: 2.6s, train loss: 1.395\n",
      "helloulanoulanoulanoulano\n",
      "\n",
      "Epoch: 35, time: 2.7s, train loss: 1.391\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 36, time: 2.7s, train loss: 1.388\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 37, time: 3.8s, train loss: 1.384\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 38, time: 2.7s, train loss: 1.380\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 39, time: 2.6s, train loss: 1.377\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 40, time: 2.9s, train loss: 1.373\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 41, time: 2.8s, train loss: 1.370\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 42, time: 4.0s, train loss: 1.367\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 43, time: 3.3s, train loss: 1.363\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 44, time: 3.4s, train loss: 1.360\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 45, time: 4.2s, train loss: 1.357\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 46, time: 3.9s, train loss: 1.354\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 47, time: 4.3s, train loss: 1.351\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 48, time: 3.0s, train loss: 1.348\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 49, time: 3.9s, train loss: 1.345\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 50, time: 3.3s, train loss: 1.342\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 51, time: 3.1s, train loss: 1.339\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 52, time: 3.3s, train loss: 1.337\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 53, time: 2.9s, train loss: 1.334\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 54, time: 3.2s, train loss: 1.331\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 55, time: 3.4s, train loss: 1.329\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 56, time: 3.4s, train loss: 1.326\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 57, time: 3.3s, train loss: 1.323\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 58, time: 3.8s, train loss: 1.321\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 59, time: 3.1s, train loss: 1.318\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 60, time: 4.0s, train loss: 1.316\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 61, time: 3.8s, train loss: 1.314\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 62, time: 2.7s, train loss: 1.311\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 63, time: 2.7s, train loss: 1.309\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 64, time: 2.6s, train loss: 1.306\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 65, time: 3.0s, train loss: 1.304\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 66, time: 3.5s, train loss: 1.302\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 67, time: 2.8s, train loss: 1.300\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 68, time: 3.1s, train loss: 1.298\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 69, time: 2.7s, train loss: 1.295\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 70, time: 3.5s, train loss: 1.293\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 71, time: 3.0s, train loss: 1.291\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 72, time: 4.3s, train loss: 1.289\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 73, time: 5.3s, train loss: 1.287\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 74, time: 4.5s, train loss: 1.285\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 75, time: 3.6s, train loss: 1.283\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 76, time: 3.5s, train loss: 1.281\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 77, time: 2.8s, train loss: 1.279\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 78, time: 4.1s, train loss: 1.277\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 79, time: 4.0s, train loss: 1.275\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 80, time: 3.7s, train loss: 1.274\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 81, time: 3.9s, train loss: 1.272\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 82, time: 3.1s, train loss: 1.270\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 83, time: 3.0s, train loss: 1.268\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 84, time: 3.3s, train loss: 1.266\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 85, time: 4.3s, train loss: 1.265\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 86, time: 3.4s, train loss: 1.263\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 87, time: 4.0s, train loss: 1.261\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 88, time: 3.2s, train loss: 1.259\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 89, time: 3.2s, train loss: 1.258\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 90, time: 3.8s, train loss: 1.256\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 91, time: 4.6s, train loss: 1.254\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 92, time: 3.6s, train loss: 1.253\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 93, time: 3.5s, train loss: 1.251\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 94, time: 3.7s, train loss: 1.250\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 95, time: 3.9s, train loss: 1.248\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 96, time: 4.5s, train loss: 1.246\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 97, time: 3.5s, train loss: 1.245\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 98, time: 3.4s, train loss: 1.243\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 99, time: 2.8s, train loss: 1.242\n",
      "hellohelanohelanohelanohe\n",
      "\n",
      "Epoch: 100, time: 3.4s, train loss: 1.240\n",
      "hellohelanohelanohelanohe\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "\n",
    "for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "    start = time.time()\n",
    "    train_loss = 0.0\n",
    "    n_batches = int(np.ceil(len(X_train) / batch_size))\n",
    "    for i in range(n_batches):\n",
    "        batch = X[i * batch_size: (i + 1) * batch_size]\n",
    "        X_batch = batch[:, :-1]\n",
    "        y_batch = batch[:, 1:].flatten()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred, _ = model.forward(X_batch)\n",
    "        loss = criterion(y_pred.view(-1, 28), y_batch)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= n_batches\n",
    "    train_losses.append(train_loss)\n",
    "    sec = time.time() - start\n",
    "    print(f'Epoch: {epoch}, time: {sec:.1f}s, train loss: {train_loss:.3f}')\n",
    "    print(generate_sentence(model) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерирует довольно странные вещи, нужно больше обучать наверно, но ошибка обучения падает, что радует"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
